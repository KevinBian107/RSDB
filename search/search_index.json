{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"System","text":"<p>The data size used is extremely large (fourty million rows of data), dealing with large dataset is one main challenge in this project.</p>"},{"location":"#structure-of-system","title":"Structure of System","text":"<pre><code>rsdb/\n\u251c\u2500\u2500 data/\n\u251c\u2500\u2500 configs/\n\u251c\u2500\u2500 features/\n\u2502   \u251c\u2500\u2500 featuring.py\n\u251c\u2500\u2500 math_formulation/\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 fpmc/\n\u2502   \u251c\u2500\u2500 tldf/\n\u251c\u2500\u2500 preprocess/\n\u2502   \u251c\u2500\u2500 data_preprocessing.py\n\u251c\u2500\u2500 recommendation.py\n\u251c\u2500\u2500 run.ipynb\n\u251c\u2500\u2500 train.py\n</code></pre>"},{"location":"#reference-sources","title":"Reference Sources","text":"<ol> <li>Dataset:<ul> <li>General Information</li> <li>Full Dataset</li> </ul> </li> <li>Models:<ul> <li>General models from textbook</li> <li>Translational Model</li> <li>Advance LSTM Model</li> </ul> </li> </ol>"},{"location":"#mathamatics-formulation-of-models","title":"Mathamatics Formulation of Models","text":"<ol> <li>Intro to FitRec Mathamatical Formulation</li> <li>Intro to Sequential Modeling</li> </ol>"},{"location":"api/","title":"API Calls","text":"<p>Running a training job with <code>tdlf</code> model: <pre><code>python rsdb/train.py --action \"train\" --model \"tdlf\"\n</code></pre></p> <p>Running a tunning job with <code>fpmc</code> model: <pre><code>python rsdb/train.py --action \"tune\" --model \"fpmc\"\n</code></pre></p> <p>Our system supports customized tunning through our yaml configs system, so all hyperparamters of tunning and training job can be tracked in the configs system.</p>"},{"location":"downstream/","title":"Application","text":""},{"location":"downstream/#expand-businesses-advertisements","title":"Expand Businesses &amp; Advertisements","text":"<p>We want to look at how user would behave like in the future, not predicting the past, we use the classical way of reconmendation. - How uninteracted <code>users</code> would inteacr with <code>gmap_id</code> then reconmand to high rating users. - Assuming that the <code>gmap_id</code> want to expand business to certain location, we are modeling how <code>users</code> in this region would interact with this <code>gmap_id</code> ().</p> <pre><code>- given gmap_id + location_range\n- for all users in this region:\n    - predict rating for each uninteracted users\n    - reconmand business to predicted high rating users\n</code></pre>"},{"location":"downstream/#start-new-businesses","title":"Start New Businesses","text":"<p>Given a <code>location (lattitude, longitute)</code> -&gt; binning -&gt; look at all business rating predictions in this bin -&gt; predict the best business for this location bin given all user rating in this location (implicit inm the recommander system). - If score of business is high, interacting with lcoal user (captured in data, think about A1 book_id and user_id), high rating tells about insights of successful business in theis region.</p> <pre><code>- bining all locations (same as feature engineering)  \n- for all users in (location_bin + hours_want_to_operate):\n    - query all the needed info (temporal info + gmap popularity) based on user info in such location\n        - all user in such location has history of interacting with certain business category\n    - predict ratings for all type of business x all user in such location\n    - aggregate all ratings grouoby business location Bin\n    - ranking\n</code></pre>"},{"location":"eda/","title":"Explorative Data Analysis","text":""},{"location":"eda/#data-cleaning","title":"Data Cleaning","text":"<p>Notice that EDA here uses a toy <code>hawaii_dataset</code>, different from the actual dataset we are using</p> <ul> <li>Traditional data science cleaning (systematic and rigorous)<ul> <li>Type conversion.</li> <li>Outlier (make it N/A?).</li> <li>Evaluate the outlier impact (use BoxPlot).</li> <li>Column split if needed.</li> <li>Narrow dataset to what we need (filter).</li> </ul> </li> <li>Missing value imputation. How do we deal with it? Depending on the case, look at the impact.<ul> <li>If missing value less than 5%, drop it directly.</li> <li>If missing value more, do dpendent random imputation.</li> </ul> </li> <li>Contraints on data<ul> <li>Standardization</li> <li>Constraint function (data integrity!!! Do we have functional dependency!!!\ud83e\udd2a)<ul> <li><code>The data is where the key lays in (Justin Eldridge)</code></li> <li>Ensure atomic typing</li> </ul> </li> </ul> </li> <li>Look at data distribution (rating,...), beware of imbalance issues.</li> <li>Prevent dropping features, return as much feature as possible.</li> </ul>"},{"location":"eda/#eexplorations-of-data","title":"Eexplorations of Data","text":"<p>Let's try to find some intelligence in the dataset.</p> <ul> <li>Dataset too big problem (how to deal with large data set)</li> <li>We do not use the subset, subsetting makes data space sparse. We use the k-core for using only the dense data set and thinking about the more active user and items.<ul> <li>Narrow the data scope (i.e. <code>entertainment</code> + <code>food</code> + <code>retail</code> using the <code>California dataset</code>)<ul> <li>Choose a state.</li> <li>Choose a few types of business.</li> <li>Choose a specific time stamp (a few year)</li> <li>Random samples may cause issues: need to ensure this sample represents the population.</li> </ul> </li> </ul> </li> <li>Find relevant features (correlation study)</li> </ul>"},{"location":"features/","title":"Features","text":""},{"location":"features/#static-attributes","title":"Static Attributes","text":"<p>Shove vectorized static attributees to model latent between features, constructing a numeric representation:</p> <ol> <li><code>Category</code><ul> <li>Bining first (if we do all combination, data is sparse)</li> <li>Count (i.e. 4 categories and see how many does id satisfied)</li> <li>Number of categories having.</li> </ul> </li> <li><code>Bined Locations</code><ul> <li>Use longitude + latitude</li> </ul> </li> <li><code>Gmap popularity score</code><ul> <li>Monthly visits</li> <li>Temporal, does not construct data lekage</li> </ul> </li> <li><code>Hours</code><ul> <li>Open or not? + total opening times</li> <li>Time interval of when it is open</li> <li>Does it open during weekend </li> <li>Total time</li> </ul> </li> </ol>"},{"location":"features/#dynamic-attributes","title":"Dynamic Attributes","text":"<p>We want to model the interaction across time, building a latent representation (it is a feature) with interactions in <code>Gmap ID</code>, <code>Reviewer ID</code>, and <code>Rating</code>.</p>"},{"location":"models/","title":"Models","text":"<p>Remanber that <code>intelligence comes from the data</code>, you should use a temporal model if your data tells you so, not just by imaginations. We have nice engineering lessons from the Netflix price model, <code>build models that is particularly designed and shaped particularly your data</code> (extract intelligence form the data), the temporal user bias is specifically designed as a parametric function to follow the frame of the data.</p>"},{"location":"models/#structure-of-models","title":"Structure of Models","text":"<pre><code>fpmc (factorized personalized markov chain)/\n\u251c\u2500\u2500 fpmc.py\n\u2502   \u2514\u2500\u2500 (factorized personalized markov chain)\n\u251c\u2500\u2500 fpmc_variants.py\n\u2502   \u2514\u2500\u2500 (our variants factorized personalized markov chain)\n\u251c\u2500\u2500 baseline.ipynb\n\u2502   \u2514\u2500\u2500 (run script of non-variants fpmc baseline)\ntdlf (temporal dynamic latent factor)/\n\u251c\u2500\u2500 latent_factor.py\n\u2502   \u2514\u2500\u2500 (baseline latent factor + neural correlative)\n\u251c\u2500\u2500 temporal_static.py\n\u2502   \u2514\u2500\u2500 (Netflix price model with static user embeddings)\n\u251c\u2500\u2500 temporal_dynamics.py\n\u2502   \u2514\u2500\u2500 (Netflix price model with dynamic user embeddings)\n\u251c\u2500\u2500 temporal_dynamics_variants.py\n\u2502   \u2514\u2500\u2500 (Variants Netflix price model with dynamic user embeddings)\n\u251c\u2500\u2500 baseline.ipynb\n\u2502   \u2514\u2500\u2500 (run scripts of non-variants tlfm baseline)\n</code></pre>"},{"location":"models/#question-in-interest","title":"Question In Interest","text":"<p>We want to do reconmandation for business owner, doing the inverse from traditional reconmandations for users. We want to see what does the users like, then we want to develop our business towards that (thinking from the business perspective).</p> <ul> <li>We are modeling interaction by connection through rating (though we can also use <code>num_reviews</code>, <code>token_counts_in_review</code>, or <code>token_counts_pos_words</code>)</li> <li>Techniqually, teh <code>user</code> for us is business owners and <code>item</code> for us is the actual users, but because the structure of the dataset, we still use the traditional perspective of deeming users as <code>user</code>.</li> <li>We want to predict unknown interaction between business and users, modeling <code>interaction</code> through <code>metric</code> (counts/rating/...) and <code>features</code>, reasoning about what business has close relationship for what user.</li> </ul>"},{"location":"models/#model-architectures","title":"Model Architectures","text":"<p>Based on the pros and cons of the model, the effect would be different and what we model is different. All model have characteristics differences, they are good for different cases. In adition, we should try to avoid python for loop, use vectorization.</p> <ul> <li>Temporal Latent Factor Model Variants (<code>TLF-V</code>):<ul> <li>It manually define long-short-term bias term for user and item by <code>binning</code> or <code>parametric functions</code>, many <code>hand-crafted</code> things that is specific to the data set working with (original model for Netflix dataset).</li> <li>Temporal modeling based on time-stamp.</li> </ul> </li> <li>Factorized Personalized Markov Chain Variants (<code>FPMC-V</code>):<ul> <li>It finds automaticlaly short temporal pattern, but fails in long term temporal pattern.</li> <li>Sequential modeling.</li> <li>Different from traditional FPMC model, we need to not only distibguish perfered and non-perfered, so the vanilla model need to have use MSE instead of BPR lost.</li> </ul> </li> <li>Recurrent Neural Network (<code>LSTM</code>):<ul> <li>It cares long term and short term and finds them automatically agonist of the dataset.</li> </ul> </li> </ul>"},{"location":"models/#testing","title":"Testing","text":"<ul> <li>Use a different state? based on a region?</li> <li>Output should be given region, check with how many ranking degree.</li> <li>Use RMSE, ACC, and R^2 for now</li> <li>Study the real business success of the predictions</li> </ul>"}]}