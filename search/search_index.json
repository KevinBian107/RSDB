{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"A Sequential Recommendation System","text":"<p>This is a Beta version system. We are currently working on imrpoving model abilities and data flows.</p> <p>We are constructing a hybrid recommender system with collaborative filters and content-based filtering. The model will use features of locations, the interaction of location and user, and the temporal evolution of the interaction to predict a user's rating of a business.</p>"},{"location":"#system-flow-of-rsdb","title":"System Flow of RSDB","text":"<p>This is the main flow of our pipeline:</p> <p></p>"},{"location":"#reference-sources","title":"Reference Sources","text":"<ol> <li>Dataset:<ul> <li>General Information</li> <li>Full Dataset</li> </ul> </li> <li>Models:<ul> <li>General models from textbook</li> <li>Translational Model</li> <li>Advance LSTM Model</li> </ul> </li> </ol>"},{"location":"#mathamatics-formulation-of-models","title":"Mathamatics Formulation of Models","text":"<ol> <li>Intro to FitRec Mathamatical Formulation</li> <li>Intro to Sequential Modeling</li> </ol>"},{"location":"api/","title":"API Calls","text":"<p>We ahve created simply interactive API calls to use our system for building a business recommender system.</p>"},{"location":"api/#notebooks","title":"Notebooks","text":"<p>We have created two notebooks for a clear visualization of our training, evaluations, and downstream applications</p>"},{"location":"api/#structure-of-system","title":"Structure of System","text":"<p>The following illsutrates the structure of our system for ease of API calling and modification of code:</p> <pre><code>rsdb/\n\u251c\u2500\u2500 data/\n\u251c\u2500\u2500 configs/\n\u251c\u2500\u2500 features/\n\u2502   \u251c\u2500\u2500 featuring.py\n\u251c\u2500\u2500 math_formulation/\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 fpmc/\n\u2502   \u251c\u2500\u2500 tldf/\n\u251c\u2500\u2500 preprocess/\n\u2502   \u251c\u2500\u2500 data_preprocessing.py\n\u251c\u2500\u2500 eval/\n\u2502   \u251c\u2500\u2500 eval_processing.py\n\u251c\u2500\u2500 recommendation.py\n\u251c\u2500\u2500 train.py\n</code></pre>"},{"location":"api/#setting-up-training","title":"Setting-up Training","text":"<p>Create a environment to work on: <pre><code>conda env create\n</code></pre></p> <p>Running a training job with <code>blf</code> model(basic latent factor model): <pre><code>python rsdb/train.py --action \"train\" --model \"blf\"\n</code></pre></p> <p>Running a training job with <code>tdlf</code> model: <pre><code>python rsdb/train.py --action \"train\" --model \"tdlf\"\n</code></pre></p> <p>Running a tunning job with <code>fpmc</code> model: <pre><code>python rsdb/train.py --action \"tune\" --model \"fpmc\"\n</code></pre></p> <p>Our system supports customized tunning through our yaml configs system, so all hyperparamters of tunning and training job can be tracked in the configs system. With the config system, we can tune and choose the hyperparameter that we want to use.</p>"},{"location":"formulation/math/","title":"Modeling Dynamics in Time","text":"<p>There are usually two approaches to tackling dynamics in time: one focuses on using temporal features (timestamps), and the other focuses on looking at the order of things (sequence). We will introduce two common approaches (one from each family) in this section.</p>"},{"location":"formulation/math/#baseline-latent-factor-blf","title":"Baseline Latent Factor (BLF)","text":"<p>For our baseline evaluation, we use a plain latent factor model that only models the interaction between user and item through rating interactions (\\( \\gamma_u \\) and \\( \\gamma_i \\)) and bias terms (user bias \\( \\beta_u \\), item bias \\( \\beta_i \\), and global bias \\( \\beta_g \\)). We also added regularization on each of the terms mentioned above, which can frame the whole optimization using MSE as follows:</p> \\[ \\arg \\min_{\\beta, \\gamma} \\sum_{u,i} \\left( \\beta_g + \\beta_u + \\beta_i + \\gamma_u \\cdot \\gamma_i - R_{u,i} \\right)^2 + \\lambda \\left[ \\sum_u \\beta_u^2 + \\sum_i \\beta_i^2 + \\sum_i \\left\\| \\gamma_i \\right\\|_2^2 + \\sum_u \\left\\| \\gamma_u \\right\\|_2^2 \\right] \\]"},{"location":"formulation/math/#factorized-personalized-markov-chain-variants-fmpc-v","title":"Factorized Personalized Markov Chain Variants (FMPC-V)","text":"<p>Factorized Personalized Markov Chains (FPMC) extends from the Markov Chain model using the first-order Markovian property. It includes both user-item interactions and item-item sequential transitions and represents them as a factorized latent factor model. The core idea of FPMC is to do two things:</p> <ol> <li>Item should match user preferences.</li> <li>The next item should be consistent with the previous item.</li> </ol> <p>To achieve this, we use the scoring function \\( f(i \\mid u, j) \\) that represents the probability of seeing item \\( i \\) given the joint probability of item \\( j \\) and the user \\( u \\). We can use tensor decomposition to reduce such a probabilistic formulation into a latent factor formulation:</p> \\[ f(i \\mid u, j) = \\underbrace{\\gamma_{ui} \\cdot \\gamma_{iu}}_{f(i \\mid u)} + \\underbrace{\\gamma_{ij} \\cdot \\gamma_{ji}}_{f(i \\mid j)} + \\underbrace{\\gamma_{uj} \\cdot \\gamma_{ju}}_{f(u, j)} \\] <p>Here: - \\( f(i \\mid u) \\): Captures the compatibility between the user \\( u \\) and the next item \\( i \\) (user-item interaction, personalized). - \\( f(i \\mid j) \\): Captures the sequential relationship between the previous item \\( j \\) and the next item \\( i \\) (Markov chain). - \\( f(u, j) \\): Captures the relationship between the user \\( u \\) and the previous item \\( j \\). This term is usually neglected because a user's compatibility with a previously rated item is trivial.</p> <p>Our variants expand upon this system and incorporate feature components into the model. The features take many formats, but for demonstration purposes, we separate them into categorical and numerical features, which are embedded into the model as follows:</p> \\[ \\begin{aligned} f(i \\mid u, j, \\mathbf{F}) = &amp;\\  \\underbrace{\\gamma_{ui} \\cdot \\gamma_{iu}}_{\\text{user/next item compatibility}} +  \\underbrace{\\gamma_{ij} \\cdot \\gamma_{ji}}_{\\text{next/previous item compatibility}} + \\underbrace{\\beta_u + \\beta_i}_{\\text{user and next-item biases}} \\\\ +  &amp;\\underbrace{\\mathbf{w}^\\top \\mathbf{F}_{\\text{cat}}}_{\\text{categorical embeddings}} + \\underbrace{\\mathbf{v}^\\top \\mathbf{F}_{\\text{num}}}_{\\text{numerical embeddings}} +  \\underbrace{b_g}_{\\text{global bias}} \\end{aligned} \\] <p>Traditionally, FPMC is optimized with Bayesian Personalized Ranking. However, since we are making a non-binary categorical prediction (rating), we use plain MSE for optimization, framed as follows:</p> \\[ \\begin{aligned} \\arg \\min_{\\gamma, \\beta, \\mathbf{w}, \\mathbf{v}} \\sum_{u,i,j}  \\Bigg(\\gamma_{ui} \\cdot \\gamma_{iu} + \\gamma_{ij} \\cdot \\gamma_{ji} +  \\beta_u + \\beta_i + \\mathbf{w}^\\top \\mathbf{F} +  \\mathbf{v}^\\top \\mathbf{F} +  \\beta_g - R_{u,i} \\Bigg)^2 \\\\ +  \\lambda \\Bigg( \\sum_u \\beta_u^2 + \\sum_i \\beta_i^2 + \\sum_u \\|\\gamma_u\\|_2^2 + \\sum_i \\|\\gamma_i\\|_2^2 + \\|\\mathbf{w}\\|_2^2 + \\|\\mathbf{v}\\|_2^2 \\Bigg) \\end{aligned} \\] <p>The features are directly embedded using embedding methods with neural networks.</p>"},{"location":"formulation/math/#temporal-dynamic-latent-factor-model-with-neural-correlative-variants-tdlf-v","title":"Temporal Dynamic Latent Factor Model With Neural Correlative Variants (TDLF-V)","text":"<p>The second approach in this system extends from the model that won the Netflix Prize. This model incorporates temporal features into a latent factor framework, specifically adding bias terms that are time-dependent (\\( \\beta_i(t) \\)) and assuming that the user latent factors (\\( \\gamma_{u,k}(t) \\)) change over time. The bias term includes both binning and periodic changes:</p> \\[ \\beta_i(t) = \\beta_i + \\beta_{i,\\text{bin}}(t) + \\beta_{i,\\text{period}}(t) \\] <p>Additionally, we use a neural correlative approach, passing latent factors through a neural network. The features (\\( \\mathbf{w}^\\top \\mathbf{F} \\)) are also incorporated into the neural network as follows:</p> \\[ f(\\gamma_u, \\gamma_i) = \\text{NN}([\\gamma_u, \\gamma_i]) \\] <p>The prediction model is formulated as:</p> \\[ \\begin{aligned} \\hat{r}_{u,i,t, \\mathbf{F}} = &amp;\\  \\underbrace{\\mu}_{\\text{Global bias}} +  \\underbrace{\\beta_i}_{\\text{Static item bias}} +  \\underbrace{\\beta_i(t)}_{\\text{Dynamic item bias}} + \\underbrace{\\beta_u}_{\\text{Static user bias}} + \\underbrace{f(\\gamma_{u,k}(t), \\gamma_{i,k})}_{\\text{Interaction score}} +  \\underbrace{\\mathbf{w}^\\top \\mathbf{F}_{\\text{item}}}_{\\text{Item-specific feature effect}} \\end{aligned} \\] <p>The optimization process is framed as follows with regularization on each component:</p> \\[ \\begin{aligned} \\arg \\min_{\\alpha, \\beta, \\gamma, \\mathbf{W}} \\sum_{u,i}  \\Bigg(&amp; \\mu + \\beta_i + \\beta_i(t) + \\beta_u + f(\\gamma_{u,k}(t), \\gamma_{i,k}) + \\mathbf{w}^\\top \\mathbf{F}_{\\text{item}} - R_{u,i} \\Bigg)^2 \\\\ &amp;+  \\lambda \\Bigg( \\sum_u \\beta_u^2 + \\sum_i \\beta_i^2 + \\sum_u \\|\\gamma_u\\|_2^2 + \\sum_i \\|\\gamma_i\\|_2^2 + \\sum \\|\\mathbf{w}\\|_2^2 \\Bigg) \\end{aligned} \\]"},{"location":"formulation/paper/","title":"RSDB Paper","text":"<p>Here is the manuscript that we formulated describing our system:</p> <p></p>"},{"location":"system/downstream/","title":"Applications","text":""},{"location":"system/downstream/#expand-businesses-advertisements","title":"Expand Businesses &amp; Advertisements","text":"<p>We want to look at how user would behave like in the future, not predicting the past, we use the classical way of reconmendation. - How uninteracted <code>users</code> would inteacr with <code>gmap_id</code> then reconmand to high rating users. - Assuming that the <code>gmap_id</code> want to expand business to certain location, we are modeling how <code>users</code> in this region would interact with this <code>gmap_id</code> ().</p> <pre><code>- given gmap_id + location_range\n- for all users in this region:\n    - predict rating for each uninteracted users\n    - reconmand business to predicted high rating users\n</code></pre>"},{"location":"system/downstream/#start-new-businesses","title":"Start New Businesses","text":"<p>Given a <code>location (lattitude, longitute)</code> -&gt; binning -&gt; look at all business rating predictions in this bin -&gt; predict the best business for this location bin given all user rating in this location (implicit inm the recommander system). - If score of business is high, interacting with lcoal user (captured in data, think about A1 book_id and user_id), high rating tells about insights of successful business in theis region.</p> <pre><code>- bining all locations (same as feature engineering)  \n- for all users in (location_bin + hours_want_to_operate):\n    - query all the needed info (temporal info + gmap popularity) based on user info in such location\n        - all user in such location has history of interacting with certain business category\n    - predict ratings for all type of business x all user in such location\n    - aggregate all ratings grouoby business location Bin\n    - ranking\n</code></pre>"},{"location":"system/eda/","title":"Explorative Data Analysis","text":""},{"location":"system/eda/#data-cleaning","title":"Data Cleaning","text":"<p>Notice that EDA here uses a toy <code>hawaii_dataset</code>, different from the actual dataset we are using. The data size used is extremely large (fourty million rows of data), dealing with large dataset is one main challenge in this project.</p> <ul> <li>Traditional data science cleaning (systematic and rigorous)<ul> <li>Type conversion</li> <li>Outlier</li> <li>Evaluate the outlier impact (use BoxPlot).</li> <li>Column split if needed.</li> <li>Narrow dataset to what we need (filter).</li> </ul> </li> <li>Missing value imputation. How do we deal with it? Depending on the case, look at the impact.<ul> <li>If missing value less than 5%, drop it directly.</li> <li>If missing value more, do dpendent random imputation.</li> </ul> </li> <li>Contraints on data<ul> <li>Standardization</li> <li>Constraint function (data integrity!!! Do we have functional dependency!!!\ud83e\udd2a)<ul> <li><code>The data is where the key lays in (Justin Eldridge)</code></li> <li>Ensure atomic typing</li> </ul> </li> </ul> </li> <li>Look at data distribution (rating,...), beware of imbalance issues.</li> <li>Prevent dropping features, return as much feature as possible.</li> </ul>"},{"location":"system/eda/#eexplorations-of-data","title":"Eexplorations of Data","text":"<p>Let's try to find some intelligence in the dataset.</p> <ul> <li>Dataset too big problem (how to deal with large data set)</li> <li>We do not use the subset, subsetting makes data space sparse. We use the k-core for using only the dense data set and thinking about the more active user and items.<ul> <li>Narrow the data scope (i.e. <code>entertainment</code> + <code>food</code> + <code>retail</code> using the <code>California dataset</code>)<ul> <li>Choose a state.</li> <li>Choose a few types of business.</li> <li>Choose a specific time stamp (a few year)</li> <li>Random samples may cause issues: need to ensure this sample represents the population.</li> </ul> </li> </ul> </li> <li>Find relevant features (correlation study)</li> </ul>"},{"location":"system/features/","title":"Features","text":""},{"location":"system/features/#static-attributes","title":"Static Attributes","text":"<p>Shove vectorized static attributees to model latent between features, constructing a numeric representation:</p> <ol> <li><code>Category</code><ul> <li>Bining first (if we do all combination, data is sparse)</li> <li>Count (i.e. 4 categories and see how many does id satisfied)</li> <li>Number of categories having.</li> </ul> </li> <li><code>Bined Locations</code><ul> <li>Use longitude + latitude</li> </ul> </li> <li><code>Gmap popularity score</code><ul> <li>Monthly visits</li> <li>Temporal, does not construct data lekage</li> </ul> </li> <li><code>Hours</code><ul> <li>Open or not? + total opening times</li> <li>Time interval of when it is open</li> <li>Does it open during weekend </li> <li>Total time</li> </ul> </li> </ol>"},{"location":"system/features/#dynamic-attributes","title":"Dynamic Attributes","text":"<p>We want to model the interaction across time, building a latent representation (it is a feature) with interactions in <code>Gmap ID</code>, <code>Reviewer ID</code>, and <code>Rating</code>.</p>"},{"location":"system/models/","title":"Models","text":"<p>Remanber that <code>intelligence comes from the data</code>, you should use a temporal model if your data tells you so, not just by imaginations. We have nice engineering lessons from the Netflix price model, <code>build models that is particularly designed and shaped particularly your data</code> (extract intelligence form the data), the temporal user bias is specifically designed as a parametric function to follow the frame of the data.</p>"},{"location":"system/models/#structure-of-models","title":"Structure of Models","text":"<pre><code>fpmc (factorized personalized markov chain)/\n\u251c\u2500\u2500 fpmc.py\n\u2502   \u2514\u2500\u2500 (factorized personalized markov chain)\n\u251c\u2500\u2500 fpmc_variants.py\n\u2502   \u2514\u2500\u2500 (our variants factorized personalized markov chain)\n\u251c\u2500\u2500 baseline.ipynb\n\u2502   \u2514\u2500\u2500 (run script of non-variants fpmc baseline)\ntdlf (temporal dynamic latent factor)/\n\u251c\u2500\u2500 latent_factor.py\n\u2502   \u2514\u2500\u2500 (baseline latent factor + neural correlative)\n\u251c\u2500\u2500 temporal_static.py\n\u2502   \u2514\u2500\u2500 (Netflix price model with static user embeddings)\n\u251c\u2500\u2500 temporal_dynamics.py\n\u2502   \u2514\u2500\u2500 (Netflix price model with dynamic user embeddings)\n\u251c\u2500\u2500 temporal_dynamics_variants.py\n\u2502   \u2514\u2500\u2500 (Variants Netflix price model with dynamic user embeddings)\n\u251c\u2500\u2500 baseline.ipynb\n\u2502   \u2514\u2500\u2500 (run scripts of non-variants tlfm baseline)\n</code></pre>"},{"location":"system/models/#question-in-interest","title":"Question In Interest","text":"<p>We want to do reconmandation for business owner, doing the inverse from traditional reconmandations for users. We want to see what does the users like, then we want to develop our business towards that (thinking from the business perspective).</p> <ul> <li>We are modeling interaction by connection through rating (though we can also use <code>num_reviews</code>, <code>token_counts_in_review</code>, or <code>token_counts_pos_words</code>)</li> <li>Techniqually, teh <code>user</code> for us is business owners and <code>item</code> for us is the actual users, but because the structure of the dataset, we still use the traditional perspective of deeming users as <code>user</code>.</li> <li>We want to predict unknown interaction between business and users, modeling <code>interaction</code> through <code>metric</code> (counts/rating/...) and <code>features</code>, reasoning about what business has close relationship for what user.</li> </ul>"},{"location":"system/models/#model-architectures","title":"Model Architectures","text":"<p>Based on the pros and cons of the model, the effect would be different and what we model is different. All model have characteristics differences, they are good for different cases as they care about different properties.</p>"},{"location":"system/models/#temporal-latent-factor-model-variants-tlf-v","title":"Temporal Latent Factor Model Variants (<code>TLF-V</code>):","text":"<ol> <li>It manually define long-short-term bias term for user and item by <code>binning</code> or <code>parametric functions</code>, many <code>hand-crafted</code> things that is specific to the data set working with (original model for Netflix dataset).</li> <li>Temporal modeling based on time-stamp.</li> <li>Very dependent and over-enginnering on a specific domain of dataset.</li> </ol>"},{"location":"system/models/#factorized-personalized-markov-chain-variants-fpmc-v","title":"Factorized Personalized Markov Chain Variants (<code>FPMC-V</code>):","text":"<ol> <li>It finds automaticlaly short temporal pattern (first order Markovian property), but fails in long term temporal pattern.</li> <li>Sequential modeling, not caring about timestamps but ordering. It is the more trivial form of token prediction.<ul> <li>Item should match user preferences.</li> <li>Next item should be consistent with previous item.</li> <li>(i.e. if users watches Harry Potter 1 and really likes it + they have the ability to watch sequel of Harry Potter 1, we can say they might like Harry Potter 2)</li> </ul> </li> <li>Knowing what a user did just last time is much more predictive than knowing what the user did for all the history.</li> <li>Different from traditional FPMC model, we need to not only distibguish perfered and non-perfered, so the vanilla model need to have use MSE instead of BPR lost.</li> </ol>"},{"location":"system/models/#further-implementations","title":"Further Implementations:","text":"<ol> <li>Transformer/Recurrent Neural Network Based Models (LSTM):<ul> <li>Surpass first order Markovian property, it includes more history and recency.</li> <li>Finding these patterns automatically agonist of the dataset.</li> </ul> </li> <li>Changing latent inner product (related to SVD, but can be changed) to eucledian distance measure (Metric embeddings for sequential recommendation).</li> </ol>"},{"location":"system/models/#testing","title":"Testing","text":"<ul> <li>Use a different state? based on a region?</li> <li>Output should be given region, check with how many ranking degree.</li> <li>Use RMSE, ACC, and R^2 for now</li> <li>Study the real business success of the predictions</li> </ul>"}]}