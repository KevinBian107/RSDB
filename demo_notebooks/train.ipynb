{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Recommendation System ⚙️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory is now: /Users/kevinb/Desktop/cse158/RSDB\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import os\n",
    "current_dir = Path.cwd().parent\n",
    "os.chdir(current_dir)\n",
    "print(f\"Current working directory is now: {Path.cwd()}\")\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from rsdb.preprocess.data_preprocessing import get_clean_review_data\n",
    "from rsdb.features.featuring import featuring_engineering\n",
    "\n",
    "url = \"https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/review-California_10.json.gz\"\n",
    "meta_url = \"https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/meta-California.json.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = get_clean_review_data(url,meta_url)\n",
    "featured_df = featuring_engineering(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rsdb.models.tdlf.temporal_dynamic_v import TemporalDynamicVariants\n",
    "from rsdb.models.fpmc.fpmc_v import FPMCVariants\n",
    "from rsdb.train import tdlf_df_to_tf_dataset, fpmc_df_to_tf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the featured data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_query = featured_df[['gmap_id', 'reviewer_id', 'rating']]\n",
    "train_df = featured_df.sample(frac=0.8, random_state=42)\n",
    "test_df = featured_df.drop(train_df.index)\n",
    "featured_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Dynamic Latent Factor Model With Neural Correlative Variants (TDLF-V)\n",
    "\n",
    "This is a model with many assumptions. Notice that item $i$ refers to the business and user $u$ refers to the user.\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{r}_{u,i,t} = \\mu + \\beta_i + \\beta_i(t) + \\beta_u + \\alpha_u \\cdot \\text{dev}_u(t) + f(\\gamma_u, \\gamma_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_i(t) = \\beta_i + \\beta_{i,\\text{bin}}(t) + \\beta_{i,\\text{period}}(t)\n",
    "$$\n",
    "\n",
    "\n",
    "**Static User/Item Bias**:\n",
    "- Static bias for item $ i $:\n",
    "  $$\n",
    "  \\beta_i = \\text{Embedding}(\\text{gmap\\_id})\n",
    "  $$\n",
    "- Static bias for user $ u $:\n",
    "  $$\n",
    "  \\beta_u = \\text{Embedding}(\\text{reviewer\\_id})\n",
    "  $$\n",
    "\n",
    "\n",
    "**Temporal User/Item Bias**:\n",
    "- Temporal bias for item $ i $ based on time bins:\n",
    "  $$\n",
    "  \\beta_i(t) = \\text{Embedding}(\\text{time\\_bin})\n",
    "  $$\n",
    "- Temporal deviation for user $ u $:\n",
    "  $$\n",
    "  \\text{dev}_u(t) = \\text{sgn}(t - \\bar{t}_u) \\cdot |t - \\bar{t}_u|^{0.4}\n",
    "  $$\n",
    "  - $ t $: Timestamp of the rating.\n",
    "  - $ \\bar{t}_u $: Mean timestamp of user $ u $'s ratings.\n",
    "  - $ \\text{sgn}(x) $: Sign function, returning $ -1 $ if $ x < 0 $, and $ 1 $ otherwise.\n",
    "- Scaled user deviation:\n",
    "  $$\n",
    "  \\alpha_u \\cdot \\text{dev}_u(t)\n",
    "  $$\n",
    "  - $ \\alpha_u $: Trainable scaling factor for user $ u $.\n",
    "\n",
    "**Latent Interaction**:\n",
    "- User embedding:\n",
    "  $$\n",
    "  \\gamma_u = \\text{Embedding}(\\text{reviewer\\_id})\n",
    "  $$\n",
    "- Item embedding:\n",
    "  $$\n",
    "  \\gamma_i = \\text{Embedding}(\\text{gmap\\_id})\n",
    "  $$\n",
    "- Interaction between user and item embeddings is the following where $ \\text{NN} $ is a dense neural network:\n",
    "  $$\n",
    "  f(\\gamma_u, \\gamma_i) = \\text{NN}([\\gamma_u, \\gamma_i])\n",
    "  $$\n",
    "\n",
    "**Final Prediction**\n",
    "\n",
    "$$\n",
    "\\gamma_{u,k}(t) = \\gamma_{u,k} + \\alpha_{u,k} \\cdot \\text{dev}_u(t) + \\gamma_{u,k,t}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{r}_{u,i,t} = \\mu + b_i + b_i(t) + b_u + \\alpha_u \\cdot \\text{dev}_u(t) + f(\\gamma_{u,k}(t), \\gamma_{i,k})\n",
    "$$\n",
    "\n",
    "**Optimization**:\n",
    "\n",
    "$$\n",
    "\\arg \\min_{\\alpha, \\beta, \\gamma} \\sum_{u,i} \\left(\\mu + b_i + b_i(t) + b_u + \\alpha_u \\cdot \\text{dev}_u(t) + f(\\gamma_{u,k}(t), \\gamma_{i,k}) - R_{u,i} \\right)^2 + \\lambda \\left[ \\sum_u \\beta_u^2 + \\sum_i \\beta_i^2 + \\sum_i \\left\\| \\gamma_i \\right\\|_2^2 + \\sum_u \\left\\| \\gamma_u \\right\\|_2^2 \\right]\n",
    "$$\n",
    "\n",
    "**Variants**:\n",
    "\n",
    "$$\n",
    "\\hat{r}_{u,i,t} = \n",
    "\\underbrace{\\mu}_{\\text{Global bias}} + \n",
    "\\underbrace{b_i}_{\\text{Static item bias}} + \n",
    "\\underbrace{b_i(t)}_{\\text{Dynamic item bias (time-dependent)}} + \n",
    "\\underbrace{b_u}_{\\text{Static user bias}} + \n",
    "\\underbrace{\\alpha_u \\cdot \\text{dev}_u(t)}_{\\text{User temporal deviation bias}} + \n",
    "\\underbrace{f(\\gamma_{u,k}(t), \\gamma_{i,k})}_{\\text{Interaction score}} + \n",
    "\\underbrace{\\mathbf{F}_{\\text{item}} \\cdot \\mathbf{W}_{\\text{item}}}_{\\text{Item-specific feature effect}}\n",
    "$$\n",
    "\n",
    "**Variants Optimization**:\n",
    "\n",
    "$$\n",
    "\\arg \\min_{\\alpha, \\beta, \\gamma, \\mathbf{W}} \\sum_{u,i} \n",
    "\\left(\n",
    "\\mu + b_i + b_i(t) + b_u + \\alpha_u \\cdot \\text{dev}_u(t) + f(\\gamma_{u,k}(t), \\gamma_{i,k}) + \\mathbf{F}_{\\text{item}} \\cdot \\mathbf{W}_{\\text{item}} - R_{u,i}\n",
    "\\right)^2 \n",
    "+ \n",
    "\\lambda \\left( \\sum_u b_u^2 + \\sum_i b_i^2 + \\sum_u \\|\\gamma_u\\|_2^2 + \\sum_i \\|\\gamma_i\\|_2^2 + \\sum \\|\\mathbf{W}\\|_2^2 \\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tdlf_df_to_tf_dataset(train_df).shuffle(1024).batch(4096)\n",
    "test_data = tdlf_df_to_tf_dataset(test_df).batch(4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo purpose\n",
    "embedding_dim = 30\n",
    "dense_units = 30\n",
    "l2_reg = 1e-3\n",
    "time_bins= 20\n",
    "model = TemporalDynamicVariants(l2_reg, dense_units, embedding_dim, data_query, time_bins)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_root_mean_squared_error\", \n",
    "    patience=10,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2, \n",
    "    decay_steps=1000, \n",
    "    decay_rate=0.8\n",
    ")\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule))\n",
    "model.fit(train_data, epochs=500, validation_data=test_data, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = model.evaluate(test_data, return_dict=True)\n",
    "print(f\"Test RMSE: {test_metrics['root_mean_squared_error']}\")\n",
    "\n",
    "predictions = []\n",
    "actual_ratings = []\n",
    "for batch in test_data:\n",
    "    predicted_ratings = model(batch).numpy()\n",
    "    actual_ratings.extend(batch[\"rating\"].numpy())\n",
    "    predictions.extend(predicted_ratings)\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "actual_ratings = np.array(actual_ratings)\n",
    "\n",
    "rmse = np.sqrt(np.mean((predictions - actual_ratings) ** 2))\n",
    "print(f\"RECHECK RMSE: {rmse}\")\n",
    "\n",
    "if np.all(actual_ratings == actual_ratings.round()):\n",
    "    correct = np.mean(predictions.round() == actual_ratings)\n",
    "    print(f\"Rounded Accuracy: {correct}\")\n",
    "else:\n",
    "    print(\"Actual ratings are not integers, skipping rounded accuracy calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorized Personalized Markov Chain Variants (FMPC-V)\n",
    "\n",
    "This is a model with less assumptions.\n",
    "\n",
    "$$\n",
    "p(i_{t+1} \\mid i_t, u) = p(i_{t+1} \\mid i_t, u)\n",
    "$$\n",
    "\n",
    "In Factorized Personalized Markov Chain (FPMC), we do this calculation by:\n",
    "\n",
    "$$\n",
    "f(i \\mid u, j) = \\underbrace{\\gamma_{ui} \\cdot \\gamma_{iu}}_{\\mathclap{f(i \\mid u)}} + \\underbrace{\\gamma_{ij} \\cdot \\gamma_{ji}}_{\\mathclap{f(i \\mid j)}} + \\underbrace{\\gamma_{uj} \\cdot \\gamma_{ju}}_{\\mathclap{f(u, j)}}.\n",
    "$$\n",
    "\n",
    "Neglecting independent terms:\n",
    "\n",
    "$$\n",
    "f(i \\mid u, j) = \n",
    "\\underbrace{\\gamma_{ui} \\cdot \\gamma_{iu}}_{\\text{user's compatibility with the next item}} + \n",
    "\\underbrace{\\gamma_{ij} \\cdot \\gamma_{ji}}_{\\text{next item's compatibility with the previous item}}\n",
    "$$\n",
    "\n",
    "For our variants:\n",
    "\n",
    "$$\n",
    "f(i \\mid u, j, \\mathbf{F}) = \n",
    "\\underbrace{\\gamma_{ui} \\cdot \\gamma_{iu}}_{\\text{user's compatibility with the next item}} + \n",
    "\\underbrace{\\gamma_{ij} \\cdot \\gamma_{ji}}_{\\text{next item's compatibility with the previous item}} + \n",
    "\\underbrace{\\beta_u + \\beta_i}_{\\text{user and next-item biases}} + \n",
    "\\underbrace{\\mathbf{w}^\\top \\mathbf{F}_{\\text{cat}}}_{\\text{categorical feature embeddings}} + \n",
    "\\underbrace{\\mathbf{v}^\\top \\mathbf{F}_{\\text{num}}}_{\\text{numerical feature embeddings}} + \n",
    "\\underbrace{b_g}_{\\text{global bias}}\n",
    "$$\n",
    "\n",
    "\n",
    "Where\n",
    "- $\\gamma_{ui}, \\gamma_{iu}, \\gamma_{ij}, \\gamma_{ji}: \\text{Embedding vectors capturing user-item and item-item interactions.}$\n",
    "- $\\beta_u, \\beta_i: \\text{Bias terms for the user and the next item.}$\n",
    "- $\\mathbf{F}_{\\text{cat}}: \\text{Categorical feature embeddings.}$\n",
    "- $\\mathbf{F}_{\\text{num}}: \\text{Dense representations of numerical features (e.g., from a dense layer).}$\n",
    "- $\\mathbf{w}, \\mathbf{v}: \\text{Learnable weights for categorical and numerical features, respectively.}$\n",
    "- $b_g: \\text{Global bias.}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = fpmc_df_to_tf_dataset(train_df).shuffle(1024).batch(4096)\n",
    "test_data = fpmc_df_to_tf_dataset(test_df).batch(4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for demo purpose\n",
    "embedding_dim = 30\n",
    "l2_reg = 1e-3\n",
    "lr = 1e-3\n",
    "model = FPMCVariants(l2_reg=l2_reg, embedding_dim=embedding_dim, data_query=data_query)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_root_mean_squared_error\", \n",
    "    patience=10,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr))\n",
    "\n",
    "history = model.fit(\n",
    "    train_data, \n",
    "    validation_data=test_data, \n",
    "    epochs=500, \n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = model.evaluate(test_data, return_dict=True)\n",
    "print(f\"Test RMSE: {test_metrics['root_mean_squared_error']}\")\n",
    "\n",
    "predictions = []\n",
    "actual_ratings = []\n",
    "for batch in test_data:\n",
    "    predicted_ratings = model(batch).numpy()\n",
    "    actual_ratings.extend(batch[\"rating\"].numpy())\n",
    "    predictions.extend(predicted_ratings)\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "actual_ratings = np.array(actual_ratings)\n",
    "\n",
    "rmse = np.sqrt(np.mean((predictions - actual_ratings) ** 2))\n",
    "print(f\"RECHECK RMSE: {rmse}\")\n",
    "\n",
    "if np.all(actual_ratings == actual_ratings.round()):\n",
    "    correct = np.mean(predictions.round() == actual_ratings)\n",
    "    print(f\"Rounded Accuracy: {correct}\")\n",
    "else:\n",
    "    print(\"Actual ratings are not integers, skipping rounded accuracy calculation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
