{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "import pandas as pd\n",
    "import keras_tuner as kt\n",
    "\n",
    "from latent_factor_model import LatentFactorModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Description\n",
    "***\n",
    "\n",
    "## Latent Factor Models + Neural Corrolative Filteration\n",
    "\n",
    "$$\n",
    "\\arg \\min_{\\alpha, \\beta, \\gamma} \\sum_{u,i} \\left( \\alpha + \\beta_u + \\beta_i + \\gamma_u \\cdot \\gamma_i - R_{u,i} \\right)^2 + \\lambda \\left[ \\sum_u \\beta_u^2 + \\sum_i \\beta_i^2 + \\sum_i \\left\\| \\gamma_i \\right\\|_2^2 + \\sum_u \\left\\| \\gamma_u \\right\\|_2^2 \\right]\n",
    "$$\n",
    "\n",
    "Single terms:\n",
    "- **Global bias, $ \\alpha $**: the overall average rating across all users and items.\n",
    "- **User bias, $ \\beta_u $**: captures the tendency of user $ u $ to rate items higher or lower than the global average.\n",
    "- **Item bias, $ \\beta_i $**: inherent popularity or quality of item $ i $.\n",
    "- **User and item latent factors, $ \\gamma_u $ and $ \\gamma_i $**: capture the latent preferences of user $ u $ and the latent characteristics of item $ i $, respectively.\n",
    "\n",
    "Combinations of terms:\n",
    "- **Prediction error**: The expression $ \\left( \\alpha + \\beta_u + \\beta_i + \\gamma_u \\cdot \\gamma_i - R_{u,i} \\right)^2 $ measures the squared difference between the predicted rating $ (\\alpha + \\beta_u + \\beta_i + \\gamma_u \\cdot \\gamma_i) $ and the actual rating $ R_{u,i} $ for user $ u $ and item $ i $.\n",
    "- **Regularization term**: The term $ \\lambda \\left[ \\sum_u \\beta_u^2 + \\sum_i \\beta_i^2 + \\sum_i \\left\\| \\gamma_i \\right\\|_2^2 + \\sum_u \\left\\| \\gamma_u \\right\\|_2^2 \\right] $ penalizes large values of the biases and latent factors to prevent overfitting. Here:\n",
    "  - $ \\sum_u \\beta_u^2 $ and $ \\sum_i \\beta_i^2 $ apply regularization to the user and item biases, respectively.\n",
    "  - $ \\sum_i \\left\\| \\gamma_i \\right\\|_2^2 $ and $ \\sum_u \\left\\| \\gamma_u \\right\\|_2^2 $ apply regularization to the latent factors of items and users, respectively.\n",
    "- **Regularization coefficient, $ \\lambda $**: This parameter controls the strength of the regularization, balancing the fit to the data with the complexity of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_querry = ...\n",
    "embedding_dim = 32\n",
    "dense_units = 32\n",
    "l2_reg = 0.0201\n",
    "model = LatentFactorModel(l2_reg, dense_units, embedding_dim, data_querry)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_root_mean_squared_error\", \n",
    "    patience=10,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4, \n",
    "    decay_steps=10000, \n",
    "    decay_rate=0.9\n",
    ")\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule))\n",
    "\n",
    "cached_train = train_data.batch(4096).cache()\n",
    "cached_test = test_data.batch(4096).cache()\n",
    "model.fit(cached_train, epochs=200, validation_data=cached_test, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = model.evaluate(cached_test, return_dict=True)\n",
    "print(f\"Test RMSE: {test_metrics['root_mean_squared_error']}\")\n",
    "\n",
    "predictions = []\n",
    "actual_ratings = []\n",
    "for batch in cached_test:\n",
    "    predicted_ratings = model(batch).numpy()\n",
    "    actual_ratings.extend(batch[\"rating\"].numpy())\n",
    "    predictions.extend(predicted_ratings)\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "actual_ratings = np.array(actual_ratings)\n",
    "\n",
    "rmse = np.sqrt(np.mean((predictions - actual_ratings) ** 2))\n",
    "print(f\"RECHECK RMSE: {rmse}\")\n",
    "\n",
    "if np.all(actual_ratings == actual_ratings.round()):\n",
    "    correct = np.mean(predictions.round() == actual_ratings)\n",
    "    print(f\"Rounded Accuracy: {correct}\")\n",
    "else:\n",
    "    print(\"Actual ratings are not integers, skipping rounded accuracy calculation.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
