{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorative Data Analysis 🐼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Goals 📊\n",
    "1. Traditional Data cleaning (outlier filtering, data characteristics, ...)\n",
    "    - Too big data size, needa way to resolve this.\n",
    "    - Depending on the question in interest (what business we want to look at, we can then say what state and data we are actually using because if we are looking at industry boom or restaurant boom, there may be differences) -> `Entertainment` + `Restaurant` + `Retail`\n",
    "2. We need to merge the DataFrames to get what we need and identify questions we are interested in.\n",
    "3. Need business analysis.\n",
    "4. Need to find temporal pattern and correlated features (clustering?)\n",
    "5. Make preprocessing pipeline / feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 🤔\n",
    "We want to do reconmandation for business owner:\n",
    "- We are doing the inverse thing: see what user like to make/develop our business towards that (先看客户，然后看开什么店好)\n",
    "- Think from the business perspective.\n",
    "- We can do one perspective then reverse it.\n",
    "- If we are modeling interaction, what interaction are we modeling? All of these serve for **dynamic attributes modeling**\n",
    "    - What is our `user`: gmap_id (lontitude and latitude)\n",
    "    - What is our `item`: user_id (?)\n",
    "    - What is `interactions`: Build connection by rating (num_reviews? token_counts_in_review? token_counts_pos_words?)\n",
    "\n",
    "## Features 🤪\n",
    "Static attributes (shove in a Factorized Machine to model latent between features):\n",
    "1. location (longitude/latitude) (address?)\n",
    "2. category (one hot)\n",
    "3. price (discrete, need one hot)\n",
    "4. hours (int)\n",
    "5. MISC (one hot)\n",
    "6. ...Text/reviews (text mining), definately useful to do, but require long time development\n",
    "\n",
    "Dynamic attributes:\n",
    "1. Models the interaction across time -> a latent representation (it is a feature)\n",
    "\n",
    "## Prediction (Let's not do Dark Magic 🪄🧙）\n",
    "**Predict unknown interaction between business and users.** New business in certain area, we predict the `overall` user metric (i.e. rating) for this new business (scoring function of your business).\n",
    "- Predict the overall user metric of certain user with the item that does not exist.\n",
    "- **If score of business is high, interacting with lcoal user (captured in data, think about A1 book_id and user_id), high rating tells about insights of successful business in theis region**.\n",
    "\n",
    "1. user = `gmap_id`\n",
    "2. model `interaction` with item = `user_id` through `metric` (counts/rating/...) and `features`.\n",
    "    - Which business has close relationship for what user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "Please do not push the data to Github (in .gitignore), this will result in large file issues. Actual data set is [here](https://cseweb.ucsd.edu/~jmcauley/datasets.html#google_local)\n",
    "\n",
    "- For data efficiency, we will be using the 10-core data. A 2-degenerate graph: each vertex has at most two neighbors to its left, so the rightmost vertex of any subgraph has degree at most two. [Reference here](https://en.wikipedia.org/wiki/Degeneracy_(graph_theory))\n",
    "\n",
    "- Let's use the small dataset for testing, using Hawaii dataset\n",
    "\n",
    "- **Data**: Data itself.\n",
    "    - The reviews provided by `users`, such as ratings, review text, and pictures.\n",
    "    - Directly informs recommendations or sentiment analysis.\n",
    "    - **Give what we are looking at from a user's perspective.**\n",
    "- **MetaData**: Data that explains about the data.\n",
    "    - `Business details`, such as name, address, average rating, category, and operational hours, describing the businesses associated with the reviews.\n",
    "    - Enriches the analysis by providing business context.\n",
    "    - **Extra information we can us  to perform joins.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes about 3min to load in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(file_path):\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data\n",
    "\n",
    "base_path = Path.cwd().parent\n",
    "file_path = base_path / \"eda\" / \"data\" / \"hawaii_data.json.gz\"\n",
    "meta_file_path = base_path / \"eda\" / \"data\" / \"hawaii_metadata.json.gz\"\n",
    "\n",
    "if file_path.exists():\n",
    "    print(f\"Loading data from: {file_path}\")\n",
    "    print(f\"Loading metadata from: {meta_file_path}\")\n",
    "    metadata = parseData(meta_file_path)\n",
    "    data = parseData(file_path)\n",
    "    print(f\"Loaded {len(data)} entries.\")\n",
    "    print(f\"Loaded {len(metadata)} entries.\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}. Please ensure the file exists at the specified location.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes 3min to load into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "meta_df = pd.DataFrame(metadata)\n",
    "print(\"DataFrame created. Here's the structure:\")\n",
    "#print(df.info())\n",
    "#print(meta_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Type Conversion'''\n",
    "# Convert to seconds if timestamps are in milliseconds\n",
    "df['time'] = df['time'] / 1000\n",
    "# Use errors='coerce' to handle invalid timestamps\n",
    "df['time'] = pd.to_datetime(df['time'], unit='s', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Merge DataFrame'''\n",
    "merged_df = df.merge(meta_df, on='gmap_id', how='inner')\n",
    "merged_df.rename(columns={'time_x': 'time', 'rating_x': 'rating', 'name_x': 'u_name', 'name_y': 'b_name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Drop Columsn'''\n",
    "columns =['pics', 'state', 'relative_results', 'url', 'resp', 'address']\n",
    "merged_df = merged_df.drop(columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers (Need to work on this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(column, df):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "num_of_reviews_outliers = detect_outliers('num_of_reviews', meta_df)\n",
    "print(num_of_reviews_outliers.shape, meta_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove outliers from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = meta_df['num_of_reviews'].quantile(0.25)\n",
    "Q3 = meta_df['num_of_reviews'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "filtered_df = meta_df.filter(\n",
    "    items=meta_df[(meta_df['num_of_reviews'] >= lower_bound) & (meta_df['num_of_reviews'] <= upper_bound)].index,\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "filtered_df.shape, meta_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Visualization\n",
    "Time distribution and Rating distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['time'], bins=50, kde=True)\n",
    "plt.title('Distribution of Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='rating', data=df)\n",
    "plt.title('Rating Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should not predict rating? This imbalance would be really hard to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(meta_df['num_of_reviews'], bins=50, kde=True)\n",
    "plt.title('Number of Pictures Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Merging\n",
    "Metadata provide inofrmation about business, merge on `gmap_id` would be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df.merge(meta_df, on='gmap_id', how='inner')\n",
    "merged_df.rename(columns={'time_x': 'time', 'rating_x': 'rating', 'name_x': 'u_name', 'name_y': 'b_name'}, inplace=True)\n",
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.rename(columns={'time_x': 'time', 'rating_x': 'rating', 'name_x': 'u_name', 'name_y': 'b_name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Popularity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews trend for top businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['b_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 5 businesses by review count\n",
    "top_businesses = merged_df['b_name'].value_counts().head(5).index\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for business in top_businesses:\n",
    "    business_reviews = merged_df[merged_df['b_name'] == business].groupby(\n",
    "        merged_df['time'].dt.to_period('M')\n",
    "    ).size()\n",
    "    business_reviews.plot(label=business)\n",
    "\n",
    "plt.title('Review Trends for Top Businesses Over Time')\n",
    "plt.xlabel('Time (Month-Year)')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.legend(title=\"Top Businesses\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.unique(merged_df[merged_df['price'].notnull()]['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant numerical columns\n",
    "numeric_data = merged_df[['avg_rating', 'num_of_reviews']]\n",
    "\n",
    "# Add a numeric encoding for 'price' if available (e.g., \"$\", \"$$\")\n",
    "if 'price' in merged_df.columns:\n",
    "    merged_df['price_encoded'] = merged_df['price'].apply(\n",
    "        lambda x: len(str(x)) if isinstance(x, str) else None\n",
    "    )\n",
    "    numeric_data['price_encoded'] = merged_df['price_encoded']\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = numeric_data.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, cmap=\"coolwarm\", annot=True, fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category popularity based on rating sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert 'category' and 'address' columns to strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['category'] = merged_df['category'].apply(lambda x: ', '.join(x) if isinstance(x, list) else x)\n",
    "merged_df['address'] = merged_df['address'].apply(lambda x: str(x) if not isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only most popular categories by total review count (using the sum of rating here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_categories = merged_df.groupby('category')['rating'].sum().sort_values(ascending=False).head(10).index\n",
    "filtered_df = merged_df[merged_df['category'].isin(popular_categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group by year and category for rating sum\n",
    "review_trends_popular = filtered_df.groupby([filtered_df['time'].dt.to_period('Y'), 'category'])['rating'].sum().unstack()\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.heatmap(review_trends_popular.T, cmap=\"Greens\", annot=True, fmt=\"g\")\n",
    "plt.title('Yearly Review Trends by Most Popular Categories')\n",
    "plt.xlabel('Time (Year)')\n",
    "plt.ylabel('Business Category')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating popularity based on counts\n",
    "Rating here doesn't matter at retreieval, just for counting, check trends in rating across time.\n",
    "- People seems to give 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_trends_popular = merged_df.groupby([filtered_df['time'].dt.to_period('Y'), 'rating'])['rating'].count().unstack()\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.heatmap(rating_trends_popular.T, cmap=\"Greens\", annot=True, fmt=\"g\")\n",
    "plt.title('Yearly Rating Trends')\n",
    "plt.xlabel('Time (Year)')\n",
    "plt.ylabel('Ratings')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popularity based on rating by address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_address = merged_df.groupby('address')['rating'].sum().sort_values(ascending=False).head(30).index\n",
    "filtered_df = merged_df[merged_df['address'].isin(popular_address)]\n",
    "review_trends_popular = filtered_df.groupby([filtered_df['time'].dt.to_period('Y'), 'address'])['rating'].sum().unstack()\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.heatmap(review_trends_popular.T, cmap=\"Greens\", annot=True, fmt=\"g\")\n",
    "plt.title('Yearly Review Trends by Most Popular Address')\n",
    "plt.xlabel('Time (Year)')\n",
    "plt.ylabel('Business Address')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "import geopandas as gpd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating/Reviews Clustering\n",
    "Clustering businesses based on average rating and number of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_data = merged_df.groupby('b_name').agg({'rating': 'mean', 'user_id': 'count'}).reset_index()\n",
    "business_data.columns = ['name', 'avg_rating', 'review_count']\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(business_data[['avg_rating', 'review_count']])\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "business_data['cluster'] = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "sns.scatterplot(x='avg_rating', y='review_count', hue='cluster', data=business_data, palette='viridis')\n",
    "plt.title('Business Clusters Based on Rating and Review Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_features = merged_df[['latitude', 'longitude']].dropna()\n",
    "dbscan = DBSCAN(eps=0.01, min_samples=10, metric='haversine').fit(geo_features)\n",
    "merged_df['geo_cluster'] = dbscan.labels_\n",
    "\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    merged_df, geometry=gpd.points_from_xy(merged_df['longitude'], merged_df['latitude'])\n",
    ")\n",
    "\n",
    "gdf.plot(column='geo_cluster', cmap='viridis', legend=True, figsize=(12, 8))\n",
    "plt.title('Geographical Clustering of Businesses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Behavior Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate user statistics\n",
    "user_stats = merged_df.groupby('user_id').agg(\n",
    "    total_reviews=('rating', 'count'),\n",
    "    avg_rating=('rating', 'mean')\n",
    ").dropna()\n",
    "scaled_user_stats = scaler.fit_transform(user_stats)\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "user_clusters = kmeans.fit_predict(scaled_user_stats)\n",
    "user_stats['cluster'] = user_clusters\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(user_stats['total_reviews'], user_stats['avg_rating'], c=user_clusters, cmap='coolwarm', alpha=0.7)\n",
    "plt.title('User Clustering by Review Behavior')\n",
    "plt.xlabel('Total Reviews')\n",
    "plt.ylabel('Average Rating Given')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Based on Text Habit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize review text\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "text_features = vectorizer.fit_transform(merged_df['text'].dropna())\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "reduced_features = pca.fit_transform(text_features.toarray())\n",
    "text_kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "text_clusters = text_kmeans.fit_predict(reduced_features)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=text_clusters, cmap='rainbow', alpha=0.7)\n",
    "plt.title('Clustering Businesses Based on Review Text Content')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something in text that we may be able to use, some hidden pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Review Counts Clustering Based on Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by category and time (e.g., yearly review counts)\n",
    "temporal_data = merged_df.groupby([merged_df['time'].dt.to_period('Y'), 'category'])['rating'].count().unstack(fill_value=0)\n",
    "\n",
    "temporal_data_normalized = temporal_data.div(temporal_data.sum(axis=1), axis=0).fillna(0).T\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "clusters = kmeans.fit_predict(temporal_data_normalized)\n",
    "temporal_data_normalized['cluster'] = clusters\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for cluster_id, ax in enumerate(axes):\n",
    "    cluster_data = temporal_data_normalized[temporal_data_normalized['cluster'] == cluster_id].drop(columns='cluster')\n",
    "    \n",
    "    cluster_data.T.plot(ax=ax, legend=False, alpha=0.7)\n",
    "    ax.set_title(f'Cluster {cluster_id} Temporal Trends')\n",
    "    ax.set_xlabel('Time (Year)')\n",
    "    ax.set_ylabel('Normalized Review Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.groupby('category').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Automatica aribitarty cluster seem to have temporal pattern hidden in the category"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
