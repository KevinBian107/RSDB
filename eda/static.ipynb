{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Explorative Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "root_dir = Path().resolve().parent\n",
    "sys.path.append(str(root_dir))\n",
    "\n",
    "\n",
    "\n",
    "import rsdb.preprocess.data_preprocessing as data_preprocessing\n",
    "import rsdb.features.featuring as featuring\n",
    "import gzip\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import string\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_colwidth', None) # display whole column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Path.cwd().parent.parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current Working Directory:\", Path.cwd().parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/review-California_10.json.gz\"\n",
    "meta_url = \"https://datarepo.eng.ucsd.edu/mcauley_group/gdrive/googlelocal/meta-California.json.gz\"\n",
    "# df = data_preprocessing.get_clean_review_data(url, meta_url)\n",
    "\n",
    "df = data_preprocessing.get_single_chunk(url, meta_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check featuring categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = featuring.featuring_category(df, ['restaurant', 'park', 'store'])\n",
    "x_temp = temp[['rating', 'isin_category_restaurant', 'isin_category_park', 'isin_category_store']]\n",
    "\n",
    "x_temp.groupby(['isin_category_restaurant', 'isin_category_park', 'isin_category_store'])['rating'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def category_count(df):\n",
    "#     \"\"\"\n",
    "#     Count the size of the 'category' array for each row in the dataframe\n",
    "#     and add a new column 'category_count'.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd.DataFrame): Input dataframe with a 'category' column containing lists.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: The updated dataframe with a new 'category_count' column.\n",
    "#     \"\"\"\n",
    "#     # Ensure 'category' column exists in the DataFrame\n",
    "#     if 'category' not in df.columns:\n",
    "#         raise ValueError(\"The DataFrame must contain a 'category' column.\")\n",
    "    \n",
    "#     # Calculate the size of the category array for each row\n",
    "#     df['category_count'] = df['category'].apply(len)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# temp_x = category_count(df)\n",
    "# df['rating'].corr(df['category_count'])\n",
    "\n",
    "\n",
    "# df[['rating', \"category_count\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Feature: location on rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featuring_locations(df: pd.DataFrame, lon_bins=20, lat_bins=20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    takes in a dataframe and divide longitude and latitude into equally\n",
    "    distributed bins\n",
    "\n",
    "    Args:\n",
    "        df: input dataframe\n",
    "        lon_bins: number of bins for longitude\n",
    "        lat_bins: number of bins for latitude\n",
    "\n",
    "    return: dataframe with bins encoded into categories\n",
    "\n",
    "    \"\"\"\n",
    "    assert \"longitude\" in df.columns, \"longitude not in the dataframe\"\n",
    "    assert \"latitude\" in df.columns, \"latitude not in the dataframe\"\n",
    "\n",
    "    # Calculate bin edges for longitude and latitude\n",
    "    lon_edges = np.linspace(df[\"longitude\"].min(), df[\"longitude\"].max(), lon_bins + 1)\n",
    "    lat_edges = np.linspace(df[\"latitude\"].min(), df[\"latitude\"].max(), lat_bins + 1)\n",
    "\n",
    "    lon_bins = pd.cut(\n",
    "        df[\"longitude\"], bins=lon_edges, labels=False, include_lowest=True\n",
    "    )\n",
    "    lat_bins = pd.cut(df[\"latitude\"], bins=lat_edges, labels=False, include_lowest=True)\n",
    "\n",
    "\n",
    "    return df.assign(lon_bin=lon_bins).assign(lat_bin=lat_bins)\n",
    "\n",
    "    # lon_feature_df = pd.get_dummies(lon_bins, prefix=\"lon_bin\", dtype=int)\n",
    "    # lat_feature_df = pd.get_dummies(lat_bins, prefix=\"lat_bin\", dtype=int)\n",
    "\n",
    "    # return pd.concat([df, lon_feature_df, lat_feature_df], axis=1).drop(\n",
    "    #     columns=[\"longitude\", \"latitude\"]\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuring_locations(df).groupby(['lon_bin', 'lat_bin'])['rating'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def milliseconds_to_years(milliseconds: int) -> float:\n",
    "    \"\"\"\n",
    "    turn milliseconds into years\n",
    "\n",
    "    Args:\n",
    "        milliseconds\n",
    "\n",
    "    return: number that convert milliseconds into years\n",
    "    \"\"\"\n",
    "    seconds = milliseconds / 1000\n",
    "    minutes = seconds / 60\n",
    "    hours = minutes / 60\n",
    "    days = hours / 24\n",
    "    years = days / 365.25  # Account for leap years\n",
    "    return years\n",
    "\n",
    "def featuring_review_counts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    takes in a dataframe and count the average review per year of each gmapid\n",
    "\n",
    "    Args:\n",
    "        pd: input dataframe\n",
    "\n",
    "    return: dataframe with bins encoded into categories\n",
    "    \"\"\"\n",
    "    assert \"review_time(unix)\" in df.columns, \"no review time\"\n",
    "    assert \"gmap_id\" in df.columns, \"needs location identifier\"\n",
    "    assert \"num_of_reviews\" in df.columns, \"needs total review counts\"\n",
    "\n",
    "    latest_review_time = df[\"review_time(unix)\"].max()\n",
    "    location_earliest_review = df.groupby([\"gmap_id\"])[\"review_time(unix)\"].min()\n",
    "    location_duration_ms =  latest_review_time - location_earliest_review\n",
    "    location_duration_yr = location_duration_ms.apply(milliseconds_to_years)\n",
    "    location_duration_yr_reviws = df[[\"gmap_id\", \"num_of_reviews\"]].merge(\n",
    "        location_duration_yr, left_on=\"gmap_id\", right_index=True\n",
    "    )\n",
    "\n",
    "    assert location_duration_yr_reviws.shape[0] == df.shape[0], \"merging issue\"\n",
    "\n",
    "    return df.assign(\n",
    "        **{'avg_review(per year)': location_duration_yr_reviws['num_of_reviews'] / location_duration_yr_reviws['review_time(unix)']}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = featuring_review_counts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['avg_review(per year)'].corr(test['avg_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rsdb.features.featuring import *\n",
    "\n",
    "\n",
    "# # Create the new columns\n",
    "# df['closed_on_weekend'] = df['hours'].apply(lambda x: is_closed_on_weekend(x))\n",
    "# df['total_hours'] = df['hours'].apply(calculate_total_hours)\n",
    "\n",
    "test12 = featuring_hours(df)#['is_closed_on_weekend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuring_engineering(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test12['weekly_operating_hours'].corr(test12['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekly bins for item bias\n",
    "data_query[\"time_bin\"] = data_query[\"review_time(unix)\"] // (\n",
    "    7 * 24 * 3600\n",
    ")  # same week in same bin\n",
    "\n",
    "# Mean timestamp for user bias\n",
    "user_mean_times = data_query.groupby(\"reviewer_id\")[\"review_time(unix)\"].mean()\n",
    "data_query[\"user_mean_time\"] = data_query[\"reviewer_id\"].map(user_mean_times)\n",
    "\n",
    "\n",
    "train_df = data_query.sample(frac=0.8, random_state=42)\n",
    "test_df = data_query.drop(train_df.index)\n",
    "\n",
    "# Calculate mean and std for normalization\n",
    "time_mean, time_std = (\n",
    "    train_df[\"review_time(unix)\"].mean(),\n",
    "    train_df[\"review_time(unix)\"].std(),\n",
    ")\n",
    "user_mean_time_mean, user_mean_time_std = (\n",
    "    train_df[\"user_mean_time\"].mean(),\n",
    "    train_df[\"user_mean_time\"].std(),\n",
    ")\n",
    "\n",
    "# Normalize the training and test data\n",
    "train_df[\"review_time(unix)\"] = (train_df[\"review_time(unix)\"] - time_mean) / time_std\n",
    "test_df[\"review_time(unix)\"] = (test_df[\"review_time(unix)\"] - time_mean) / time_std\n",
    "train_df[\"user_mean_time\"] = (\n",
    "    train_df[\"user_mean_time\"] - user_mean_time_mean\n",
    ") / user_mean_time_std\n",
    "test_df[\"user_mean_time\"] = (\n",
    "    test_df[\"user_mean_time\"] - user_mean_time_mean\n",
    ") / user_mean_time_std\n",
    "\n",
    "\n",
    "# Sort data by user and timestamp and reate sequences\n",
    "\n",
    "# sort dataframe and shift the \n",
    "\n",
    "\n",
    "data = merged_clean.sort_values(by=[\"reviewer_id\", \"review_time(unix)\"])\n",
    "data[\"prev_item_id\"] = data.groupby(\"reviewer_id\")[\"gmap_id\"].shift(1)\n",
    "data = data.dropna(subset=[\"prev_item_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_top_words_by_period_sampled(df, column, time_column, periods, sample_size=10000):\n",
    "    \"\"\"\n",
    "    Finds the top 10 most common words for each time period using a sampled subset of the data.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Input DataFrame.\n",
    "        column (str): Column containing the text data.\n",
    "        time_column (str): Column containing Unix time (in milliseconds).\n",
    "        periods (list of tuples): List of (start, end) time ranges for filtering.\n",
    "        sample_size (int): Number of rows to sample for each period. Default is 10,000.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with periods as keys and top 10 words as values.\n",
    "    \"\"\"\n",
    "    # Initialize a dictionary to store results\n",
    "    top_words_by_period = {}\n",
    "\n",
    "    # Predefine stopwords and punctuation\n",
    "    stop_words = set([\n",
    "        'the', 'and', 'a', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'it', 'this', 'is', 'from', 'by', 'as', 'that', 'which', 'or', 'an'\n",
    "        \n",
    "    ])\n",
    "    punctuations = set(string.punctuation)\n",
    "\n",
    "    for start, end in periods:\n",
    "        # Filter DataFrame for the time period\n",
    "        period_df = df[(df[time_column] >= start) & (df[time_column] < end)]\n",
    "\n",
    "        # Sample a subset of the data\n",
    "        sampled_df = period_df.sample(n=min(sample_size, len(period_df)), random_state=42)\n",
    "\n",
    "        # Combine all text into one string from the sampled data\n",
    "        all_text = \" \".join(sampled_df[column].dropna().astype(str).tolist())\n",
    "\n",
    "        # Tokenize words, convert to lowercase, and remove stopwords/punctuation\n",
    "        words = [\n",
    "            word.lower() for word in all_text.split()\n",
    "            if word.lower() not in stop_words and word not in punctuations\n",
    "        ]\n",
    "\n",
    "        # Count word frequencies and get the top 10\n",
    "        word_counts = Counter(words)\n",
    "        top_words = word_counts.most_common(10)\n",
    "\n",
    "        # Store results for this period\n",
    "        top_words_by_period[f\"{start}-{end}\"] = top_words\n",
    "\n",
    "    return top_words_by_period\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time periods (Unix time in milliseconds)\n",
    "periods = [\n",
    "    (1104537600 * 1000, 1262304000 * 1000),  # 2005-2010\n",
    "    (1262304000 * 1000, 1420070400 * 1000),  # 2010-2015\n",
    "    (1420070400 * 1000, 1577836800 * 1000),  # 2015-2020\n",
    "    (1577836800 * 1000, 1704067200 * 1000),  # 2020-2024\n",
    "]\n",
    "\n",
    "# Find top words for each period, with sampling\n",
    "top_words = find_top_words_by_period_sampled(df, column='text', time_column='review_time(unix)', periods=periods, sample_size=10000)\n",
    "\n",
    "# Print results\n",
    "for period, words in top_words.items():\n",
    "    print(f\"Top words for {period}:\")\n",
    "    for word, count in words:\n",
    "        print(f\"{word}: {count}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def average_rating_maps(df, time_column, rating_column, periods, sample_size=100000, location_column=\"latitude, longitude\"):\n",
    "    \"\"\"\n",
    "    Generates maps showing the average rating of all locations over time.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame containing the review data.\n",
    "        time_column (str): Column containing Unix time (in milliseconds).\n",
    "        rating_column (str): Column containing the review ratings.\n",
    "        periods (list of tuples): List of (start, end) time ranges for filtering.\n",
    "        sample_size (int): Number of rows to sample from the entire DataFrame. Default is 100,000.\n",
    "        location_column (str): Column for the location. Assumes a tuple for (latitude, longitude).\n",
    "        \n",
    "    Returns:\n",
    "        List of Folium Maps for each time period.\n",
    "    \"\"\"\n",
    "    # Sample the dataframe once\n",
    "    sampled_df = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "    # List to store the maps\n",
    "    maps = []\n",
    "    \n",
    "    for i, (start, end) in enumerate(periods):\n",
    "        # Filter the data for the current period\n",
    "        period_df = sampled_df[(sampled_df[time_column] >= start) & (sampled_df[time_column] < end)]\n",
    "\n",
    "        # Group by location (latitude and longitude) and calculate the average rating for each location\n",
    "        avg_ratings_per_location = period_df.groupby(['latitude', 'longitude'])[rating_column].mean().reset_index()\n",
    "\n",
    "        # Create a Folium map centered around a mean location\n",
    "        avg_lat = avg_ratings_per_location['latitude'].mean()\n",
    "        avg_lon = avg_ratings_per_location['longitude'].mean()\n",
    "        m = folium.Map(location=[avg_lat, avg_lon], zoom_start=6)\n",
    "\n",
    "        # Create a marker cluster\n",
    "        marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "        # Add a marker for each location with average rating\n",
    "        for _, row in avg_ratings_per_location.iterrows():\n",
    "            folium.CircleMarker(\n",
    "                location=[row['latitude'], row['longitude']],\n",
    "                radius=5,\n",
    "                popup=f\"Avg Rating: {row[rating_column]:.2f}\",\n",
    "                color='blue',\n",
    "                fill=True,\n",
    "                fill_color='blue',\n",
    "                fill_opacity=0.6\n",
    "            ).add_to(marker_cluster)\n",
    "        \n",
    "        # Title for the map based on the period\n",
    "        period_label = f\"Average Rating (from {pd.to_datetime(start, unit='ms').year} to {pd.to_datetime(end, unit='ms').year})\"\n",
    "        maps.append(m)\n",
    "        m.save(f\"average_rating_map_{i}.html\")  # Save map as HTML file\n",
    "    \n",
    "    return maps\n",
    "\n",
    "# Define time periods (Unix time in milliseconds)\n",
    "periods = [\n",
    "    (1104537600 * 1000, 1262304000 * 1000),  # 2005-2010\n",
    "    (1262304000 * 1000, 1420070400 * 1000),  # 2010-2015\n",
    "    (1420070400 * 1000, 1577836800 * 1000),  # 2015-2020\n",
    "    (1577836800 * 1000, 1704067200 * 1000),  # 2020-2024\n",
    "]\n",
    "\n",
    "# Call the function to generate the maps\n",
    "maps = average_rating_maps(df, time_column='review_time(unix)', rating_column='avg_rating', periods=periods, sample_size=100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Sample 1000 entries from the dataframe\n",
    "df_sample = df.sample(n=1000, random_state=42)\n",
    "\n",
    "# Extract latitude, longitude, and category for each entry\n",
    "df_sample['latitude'] = df_sample['latitude']\n",
    "df_sample['longitude'] = df_sample['longitude']\n",
    "df_sample['category'] = df_sample['category']\n",
    "\n",
    "# Create a function to get the most popular category for each location\n",
    "def get_most_popular_category(location_df):\n",
    "    # Count the occurrences of each category for a given location\n",
    "    category_counts = location_df['category'].explode().value_counts()\n",
    "    if not category_counts.empty:\n",
    "        return category_counts.idxmax()  # Get the most frequent category\n",
    "    return None\n",
    "\n",
    "# Group by location and get the most popular category for each location\n",
    "location_categories = df_sample.groupby(['latitude', 'longitude']).apply(get_most_popular_category).reset_index(name='most_popular_category')\n",
    "\n",
    "# Create the map centered around an average location (or any central location)\n",
    "map_center = [df_sample['latitude'].mean(), df_sample['longitude'].mean()]\n",
    "mymap = folium.Map(location=map_center, zoom_start=10)\n",
    "\n",
    "# Add markers for each location with the most popular category\n",
    "for idx, row in location_categories.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        popup=f\"Category: {row['most_popular_category']}\",\n",
    "        icon=folium.Icon(color='blue', icon='info-sign')\n",
    "    ).add_to(mymap)\n",
    "\n",
    "# Save the map to an HTML file and display it\n",
    "mymap.save(\"most_popular_categories_map.html\")\n",
    "\n",
    "# To display the map inline in Jupyter or similar environments, you can use:\n",
    "# mymap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 10000 entries from the dataframe\n",
    "sample_size = 10000\n",
    "df_sample = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Define latitude and longitude bounds\n",
    "lat_min, lat_max = 32.5, 42\n",
    "long_min, long_max = -124.4, -114.13\n",
    "\n",
    "# Filter out rows where latitude and longitude are outside the bounds\n",
    "df_sample_filtered = df_sample[\n",
    "    (df_sample['latitude'] >= lat_min) & (df_sample['latitude'] <= lat_max) & \n",
    "    (df_sample['longitude'] >= long_min) & (df_sample['longitude'] <= long_max)\n",
    "]\n",
    "\n",
    "# Create a function to get the most popular category for each location\n",
    "def get_most_popular_category(location_df):\n",
    "    # Count the occurrences of each category for a given location\n",
    "    category_counts = location_df['category'].explode().value_counts()\n",
    "    if not category_counts.empty:\n",
    "        return category_counts.idxmax()  # Get the most frequent category\n",
    "    return None\n",
    "\n",
    "# Group by location and get the most popular category for each location\n",
    "location_categories = df_sample_filtered.groupby(['latitude', 'longitude']).apply(get_most_popular_category).reset_index(name='most_popular_category')\n",
    "\n",
    "# Merge the most_popular_category column back into df_sample_filtered\n",
    "df_sample_filtered = pd.merge(df_sample_filtered, location_categories, on=['latitude', 'longitude'], how='left')\n",
    "\n",
    "# Get the top 10 most popular categories\n",
    "top_categories = df_sample_filtered['most_popular_category'].value_counts().nlargest(10).index\n",
    "\n",
    "# Filter the dataframe to include only the top 10 categories\n",
    "df_sample_filtered = df_sample_filtered[df_sample_filtered['most_popular_category'].isin(top_categories)]\n",
    "\n",
    "# Map each category to a color\n",
    "category_colors = {category: color for category, color in zip(df_sample_filtered['most_popular_category'].unique(), sns.color_palette(\"Set2\", len(df_sample_filtered['most_popular_category'].unique())))}\n",
    "\n",
    "# Create the plot (only one subplot for the most popular category distribution)\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Scatterplot for Most Popular Category Distribution\n",
    "sns.scatterplot(\n",
    "    data=df_sample_filtered,\n",
    "    x=\"longitude\", \n",
    "    y=\"latitude\", \n",
    "    hue=\"most_popular_category\", \n",
    "    palette=category_colors,\n",
    "    size=\"num_of_reviews\", \n",
    "    sizes=(20, 200), \n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "plt.title('Top 10 Most Popular Category Distribution (Sample)', fontsize=14)\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "\n",
    "# Move the legend outside the plot\n",
    "plt.legend(title=\"Category\", loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample 10000 entries from the dataframe\n",
    "sample_size = 10000\n",
    "df_sample = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Define latitude and longitude bounds\n",
    "lat_min, lat_max = 32.5, 42\n",
    "long_min, long_max = -124.4, -114.13\n",
    "\n",
    "# Filter out rows where latitude and longitude are outside the bounds\n",
    "df_sample_filtered = df_sample[\n",
    "    (df_sample['latitude'] >= lat_min) & (df_sample['latitude'] <= lat_max) & \n",
    "    (df_sample['longitude'] >= long_min) & (df_sample['longitude'] <= long_max)\n",
    "]\n",
    "\n",
    "# Create a function to get the most popular category for each location\n",
    "def get_most_popular_category(location_df):\n",
    "    # Count the occurrences of each category for a given location\n",
    "    category_counts = location_df['category'].explode().value_counts()\n",
    "    if not category_counts.empty:\n",
    "        return category_counts.idxmax()  # Get the most frequent category\n",
    "    return None\n",
    "\n",
    "# Group by location and get the most popular category for each location\n",
    "location_categories = df_sample_filtered.groupby(['latitude', 'longitude']).apply(get_most_popular_category).reset_index(name='most_popular_category')\n",
    "\n",
    "# Merge the most_popular_category column back into df_sample_filtered\n",
    "df_sample_filtered = pd.merge(df_sample_filtered, location_categories, on=['latitude', 'longitude'], how='left')\n",
    "\n",
    "# Get the top 20 most popular categories\n",
    "top_categories = df_sample_filtered['most_popular_category'].value_counts().nlargest(20).index\n",
    "\n",
    "# Filter the dataframe to include only the top 20 categories\n",
    "df_sample_filtered = df_sample_filtered[df_sample_filtered['most_popular_category'].isin(top_categories)]\n",
    "\n",
    "# Map each category to a color\n",
    "category_colors = {category: color for category, color in zip(df_sample_filtered['most_popular_category'].unique(), sns.color_palette(\"Set2\", len(df_sample_filtered['most_popular_category'].unique())))}\n",
    "\n",
    "# Create the plot (only one subplot for the most popular category distribution)\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Scatterplot for Most Popular Category Distribution\n",
    "sns.scatterplot(\n",
    "    data=df_sample_filtered,\n",
    "    x=\"longitude\", \n",
    "    y=\"latitude\", \n",
    "    hue=\"most_popular_category\", \n",
    "    palette=category_colors,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "plt.title('Top 20 Most Popular Category Distribution (Sample)', fontsize=14)\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "\n",
    "# Move the legend outside the plot\n",
    "plt.legend(title=\"Category\", loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 50000 entries from the dataframe\n",
    "sample_size = 50000\n",
    "df_sample = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Define latitude and longitude bounds\n",
    "lat_min, lat_max = 32.5, 42\n",
    "long_min, long_max = -124.4, -114.13\n",
    "\n",
    "# Filter out rows where latitude and longitude are outside the bounds\n",
    "df_sample_filtered = df_sample[\n",
    "    (df_sample['latitude'] >= lat_min) & (df_sample['latitude'] <= lat_max) & \n",
    "    (df_sample['longitude'] >= long_min) & (df_sample['longitude'] <= long_max)\n",
    "]\n",
    "\n",
    "# Filter the dataframe for the selected categories\n",
    "categories_of_interest = ['Shopping mall', 'Grocery store', 'Restaurant', 'Movie theater']\n",
    "df_sample_filtered = df_sample_filtered[df_sample_filtered['category'].apply(lambda x: any(cat in categories_of_interest for cat in x))]\n",
    "\n",
    "# Explode the categories column to work with individual categories\n",
    "df_sample_filtered_exploded = df_sample_filtered.explode('category')\n",
    "\n",
    "# Keep only the rows where the category is in the list of interest\n",
    "df_sample_filtered_exploded = df_sample_filtered_exploded[df_sample_filtered_exploded['category'].isin(categories_of_interest)]\n",
    "\n",
    "# Create a color palette for the categories\n",
    "category_colors = {\n",
    "    'Shopping mall': 'blue',\n",
    "    'Grocery store': 'green',\n",
    "    'Restaurant': 'red',\n",
    "    'Movie theater': 'purple'\n",
    "}\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatterplot for selected categories\n",
    "sns.scatterplot(\n",
    "    data=df_sample_filtered_exploded,\n",
    "    x=\"longitude\", \n",
    "    y=\"latitude\", \n",
    "    hue=\"category\", \n",
    "    palette=category_colors,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title('Distribution of Shopping Mall, Grocery Store, Restaurant, and Movie Theater', fontsize=14)\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "\n",
    "# Move the legend outside the plot\n",
    "plt.legend(title=\"Category\", loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1000 entries from the dataframe (adjust the sample size as needed)\n",
    "sample_size = 300000\n",
    "df_sample = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Assuming you have a 'review_time' column or similar that contains datetime or Unix timestamps\n",
    "# If the column is a Unix timestamp (in milliseconds), we first need to convert it to a datetime object\n",
    "df_sample['review_time'] = pd.to_datetime(df_sample['review_time(unix)'], unit='ms')\n",
    "\n",
    "# Extract the year from the 'review_time' column\n",
    "df_sample['year'] = df_sample['review_time'].dt.year\n",
    "\n",
    "# Filter the dataset to include only the specific business types we're interested in\n",
    "business_types = ['Restaurant', 'Shopping mall', 'Park', 'Grocery store', 'Movie theater']\n",
    "df_sample_filtered = df_sample[df_sample['category'].apply(lambda x: any(business in x for business in business_types))]\n",
    "\n",
    "# Create a new column that maps the specific business types\n",
    "def map_business_type(categories):\n",
    "    for business in business_types:\n",
    "        if business in categories:\n",
    "            return business\n",
    "    return None  # In case no category matches\n",
    "\n",
    "df_sample_filtered['business_type'] = df_sample_filtered['category'].apply(map_business_type)\n",
    "\n",
    "# Group by year and business type, then count the number of businesses for each\n",
    "businesses_per_year_type = df_sample_filtered.groupby(['year', 'business_type']).size().reset_index(name='count')\n",
    "\n",
    "# Create the line plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Create a line plot for each business type\n",
    "sns.lineplot(x='year', y='count', hue='business_type', data=businesses_per_year_type, marker='o')\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title('Number of Businesses by Type Over Time', fontsize=14)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Businesses')\n",
    "\n",
    "# Display the legend outside the graph\n",
    "plt.legend(title=\"Business Type\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Adjust x-axis to display every year, not just specific intervals\n",
    "plt.xticks(businesses_per_year_type['year'].unique(), rotation=45)  # Ensure each year is shown\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Find unique keys in the dictionaries within the MISC column\n",
    "unique_keys = set()\n",
    "\n",
    "# Iterate through each dictionary in the 'MISC' column and collect the keys\n",
    "for misc_dict in df['MISC']:\n",
    "    if isinstance(misc_dict, dict):  # Ensure the value is a dictionary\n",
    "        unique_keys.update(misc_dict.keys())\n",
    "\n",
    "# Print unique keys\n",
    "print(\"Unique keys in MISC:\", unique_keys)\n",
    "\n",
    "# Step 2: Find unique values for each key (handling lists as values)\n",
    "unique_values = {}\n",
    "\n",
    "# Iterate through each dictionary again to get unique values for each key\n",
    "for key in unique_keys:\n",
    "    # Collect unique values for the current key\n",
    "    values_for_key = set()\n",
    "    for misc_dict in df['MISC']:\n",
    "        if isinstance(misc_dict, dict) and key in misc_dict:\n",
    "            # Add each item from the list to the set for unique values\n",
    "            values_for_key.update(misc_dict[key])\n",
    "    unique_values[key] = values_for_key\n",
    "\n",
    "# Print the number of unique values for each key\n",
    "for key, values in unique_values.items():\n",
    "    print(f\"Key: {key} has {len(values)} unique values: {values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a separate DataFrame for one-hot encoded columns\n",
    "one_hot_encoded_df = pd.DataFrame(index=df.index)  # Use the same index as the original df\n",
    "\n",
    "# Iterate through each row in the dataframe to check for key-value presence\n",
    "for index, row in df.iterrows():\n",
    "    if isinstance(row['MISC'], dict):\n",
    "        for key, values in unique_values.items():\n",
    "            # For each key, iterate through the possible values\n",
    "            for value in values:\n",
    "                # Create a new column name based on key-value\n",
    "                column_name = f\"{key}_{value}\"\n",
    "                \n",
    "                # Check if the value exists for the key in the dictionary\n",
    "                if key in row['MISC'] and value in row['MISC'][key]:\n",
    "                    # If present, set the column to 1\n",
    "                    one_hot_encoded_df.at[index, column_name] = 1\n",
    "                else:\n",
    "                    # If not present, set the column to 0\n",
    "                    one_hot_encoded_df.at[index, column_name] = 0\n",
    "\n",
    "# Step 3: Final clean-up: Replace NaN with 0 (since new columns may have NaN values initially)\n",
    "one_hot_encoded_df = one_hot_encoded_df.fillna(0)\n",
    "\n",
    "# Now, one_hot_encoded_df contains the one-hot encoded columns\n",
    "print(one_hot_encoded_df.head())  # See the result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse158",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
